<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Predictive regressions:</title>
    <meta charset="utf-8" />
    <meta name="author" content="Guillaume Coqueret (.font90[joint work with Romain Deguest])" />
    <link href="slides_files/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="slides_files/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <script src="slides_files/kePrint-0.0.1/kePrint.js"></script>
    <link href="slides_files/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
    <link rel="stylesheet" href="css/extra.css" type="text/css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Predictive regressions:
## a machine learning perspective
### Guillaume Coqueret (.font90[joint work with Romain Deguest])
### .font90[<strong>EMLYON</strong> Presentation] 2020-10-22

---


&lt;!-- xaringan::inf_mr() --&gt;

class: inverse, center, middle, animated, fadeInRight


&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 32px;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

# Introduction

---
class: animated, fadeInRight


# PRs: the traditional approach

.font105[
Given two processes `\(y_t\)` and `\(x_t\)`, we define as PR the following model:

`$$y_{t+k} = a + b x_t + e_{t+k},$$`

where both `\(a\)` and `\(b\)` must be estimated (given some sample). We call `\(k\)` the forecasting horizon.

**Traditionally**, assumptions are made on `\(x_t\)` and on the error term, and statistical properties of estimators `\(\hat{b}\)` of `\(b\)` are inferred.
]

---
# The ML setting

Once `\((a,b)\)` has been estimated, the natural prediction for `\(y_{t+k}\)` is `\(\tilde{y}_{t+k} = \hat{a}+\hat{b}x_t\)` and the usual way to measure its .bluep[**accuracy**] is to compute the (out-of-sample) .bluep[Mean Squared Error] (MSE), or quadratic loss:

`$$L = \mathbb{E}\left[(y_{t+k}-\tilde{y}_{t+k})^2\right].$$`

Theoretically, this is a hard problem, because even in the simplest (**OLS**) case, the estimator `\(\hat{b}\)` has a pretty intractable expression:
`$$\small \hat{b}=\frac{\sum_{s=0}^{T-1}(x_{t-k-s}-\bar{x})(y_{t-s}-\bar{y})}{\sum_{s=0}^{T-1}(x_{t-k-s}-\bar{x})^2} \quad (\text{all terms are random})$$`

---
# The simple case

If all parameters are .bluep[**known**], then the loss is minimized by the **optimal slope**:
`$$b_o=\frac{\text{cov}(x_t,y_{t+k})}{\text{var}(x_t)}=\rho_y^{k}\frac{\rho\sigma_x\sigma_y}{1-\rho_x\rho_y}\frac{1-\rho_x^2}{\sigma_x^2},$$`
in which case `\(\small L_o=\frac{\sigma_y^2}{1-\rho_y^2}\,\left(1-\rho_y^{2k}\frac{\rho^2}{(1-\rho_x\rho_y)^2}(1-\rho_x^2)(1-\rho_y^2)\right)\)` and
`$$R^2_o=\rho_y^{2k}\frac{\rho^2}{(1-\rho_x\rho_y)^2}(1-\rho_x^2)(1-\rho_y^2),$$`
which increases with `\(|\rho|\)`, and is always .bluep[**nonnegative**].


---
# But, in real life...

.font110[Parameters are **unknown**!]


`\(\rightarrow\)` .font110[in practice, the easiest (most common) thing to do is to use the .bluep[**OLS estimator**]. It is the route we take.]    
`\(\rightarrow\)` .font110[however, it is far from obvious that this is an optimal choice.]   
`\(\rightarrow\)` .font110[in fact, extensive testing would show that it can lead to negative (out-of-sample)] `\(R^2\)` !


---
# The assumptions

In the paper, we assume that both `\(x\)` and `\(y\)` are **AR**(1) **processes**:
`\begin{align}
x_{t+1} &amp;= \alpha_x + \rho_x x_t +e_{x,t+1},
\label{eq:ar_x}\\
y_{t+1} &amp;= \alpha_y + \rho_y y_t +e_{y,t+1},
\label{eq:ar_y}     
\end{align}`
with constants `\(\alpha_x\)` and `\(\alpha_y\)`, autocorrelations `\(\rho_x\)` and `\(\rho_y\)` satisfying `\(|\rho_x|&lt;1\)` and `\(|\rho_y|&lt;1\)`, and **correlated Gaussian white noise processes** `\(e_{x}\)` and `\(e_{y}\)` with variances `\(\sigma_x^2\)` and `\(\sigma_y^2\)` and correlation `\(\rho\)` satisfying `\(|\rho|&lt;1\)`.

These hypotheses are .bluep[rather common] in the literature.

---
# A few references on PRs


.bib[
.font80[
- .bluep[**Stambaugh JFE 1999**]: sample estimates are biased when `\(x\)` is persistent   
- .bluep[**Bandi et al. J. Econom 2019**]: predictability attains its peak around 12-15Y   
- Extensions of Stambaugh (panel &amp; multivariate): 
 * .bluep[**Amihud &amp; Hurvich JFQA 2004**], 
 * .bluep[**Hjalmarrson FRL 2008**], 
 * .bluep[**Andersen &amp; Varneskov J. Econom 2020**]    
- Long horizon estimates are skewed by construction: 
 * .bluep[**Valkanov JFE 2003**], 
 * .bluep[**Boudoukh et al. RFS 2008**]      
- Efficient tests for inference: 
 * .bluep[**Campbell &amp; Yogo JFE 2006**], 
 * .bluep[**Jonhson RAPS 2019**], 
 * .bluep[**Xu RFS 2020**]

]
]

---
# In short 

    
&lt;img src="summary.svg" width="960px" style="display: block; margin: auto;" /&gt;



---
# Our research question

.font120[**When is it a good idea to use the OLS estimator in predictive regressions?**] (i.e. when is `\(R^2&gt;0\)`?)

The (complicated) answer depends on the .bluep[**model parameters**]:
- `\(k\)`, the horizon of the forecast;   
- `\(T\)`*, the sample size;    
- `\(\rho_x\)`, `\(\rho_y\)`: the autocorrelations;    
- `\(\rho\)`: the (cross-)correlation in innovations.   
- `\(\alpha_x\)`, `\(\alpha_y\)`, `\(\sigma_x^2\)`, `\(\sigma_y^2\)`... ?


---

class: inverse, center, middle, animated, fadeInRight


&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 32px;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

# Theoretical results



---
# Simple facts

The loss (mean squared error) has the following .bluep[**properties**]:   

- it does not depend on the constants of the processes `\(\alpha_x\)` &amp; `\(\alpha_y\)`;   
- it does not depend on `\(\sigma_x^2\)`, the variance of innovations of `\(x\)`;      
- it is **proportional** to `\(\sigma_y^2\)`, the variance of innovations of `\(y\)`;    
- it is **symmetric** in `\(\rho\)`, the correlation between the two innovation processes.

Moreover, if means are known, `\(\rho=0\)` and `\(\rho_x=\rho_y\)`, then we can show that **short term forecasting is more accurate**, i.e., that

`$$L(k+1) &gt; L(k), \quad k \ge 1 .$$`


---
# Analytical forms

The loss can be written

`$$L=\sigma_y^2 \left(v_y +  \int_0^\infty |\boldsymbol{\Delta}| \, \left(  t \, f_2(t)-2f_1(t)\right) dt \right),$$`
where `\(v_y\)` is a simple variance term related to `\(y_t\)`. The matrix `\(\boldsymbol{\Delta}\)` and the functions `\(f_1\)` and `\(f_2\)` depend on `\(k\)` (horizon), on `\(T\)` (sample size) and on the correlation parameters ( `\(\rho_x\)`, `\(\rho_x\)` and `\(\rho\)`) in a very non-trivial fashion.


---
# Sensitivity: 1 month horizon forecasting

   
&lt;img src="sensi_k_01.svg" width="680px" style="display: block; margin: auto;" /&gt;


.font80[Big change for **high persistence** in *y* (right column, `\(L_o\)` not the best!).   
Persistence in *x* matters less.]





---
# Sensitivity: 12 month horizon forecasting

   
&lt;img src="sensi_k_12.svg" width="700px" style="display: block; margin: auto;" /&gt;

.font85[In all cases, **long samples** are preferable.]



---
# The memory tradeoff (1/2)

**Best combo**: low `\(k\)`, high `\(\rho_y\)`. In practice: not that easy!

&lt;img src="scheme2.svg" width="700px" style="display: block; margin: auto;" /&gt;




---
# The memory tradeoff (2/2)

Illustration on the US **equity premium** (S&amp;P 500 minus TBill).

&lt;img src="memory-trade_fit.svg" width="800px" style="display: block; margin: auto;" /&gt;

**Model obsolescence** seems to outweigh persistence gains.


---
# Convergence towards `\(b_o\)` and `\(R^2_o\)`

As `\(T\)` increases, the loss/ `\(R^2\)` converges to some limiting value: it's the **theoretical optimum** (OLS)! Again, **persistence ** is key!

&lt;img src="convergence.svg" width="840px" style="display: block; margin: auto;" /&gt;


---

class: inverse, center, middle, animated, fadeInRight


&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 32px;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

# Empirical confirmation


---
# Data

We work on the *Welch &amp; Goyal* (2008) dataset (updated through Dec. 2019) + a temperature variable from *Novy-Marx* (2014).

Predictors include aggregate **dividend-price** ratio, **book-to-market** ratio, **earnings-price** ratio, **stock variance**, etc. + some variations thereof. We categorize them in 3 groups of **persistence** (low, medium, high).

The dependent variable is the US Equity premium, with **horizons** of 1, 3, 6, 12 and 24 months.

---
# The impact of persistence

Low persistence in predictors works better. But still, `\(R^2&lt;0\)`!

&lt;img src="rhos.svg" width="890px" style="display: block; margin: auto;" /&gt;




---
# Economic constraints

.font80[The equity premium is **positive** (most often): cut the negative part of predictions!]

&lt;img src="constraints.svg" width="800px" style="display: block; margin: auto;" /&gt;

---
# Portfolio timing

.font90[A signal that aggregates individual predictors. Sharpe ratios (TC adj.):]

&lt;table class=" lightable-paper lightable-hover table table-striped lightable-material lightable-striped lightable-hover" style='font-family: Cambria; margin-left: auto; margin-right: auto; font-size: 18px; margin-left: auto; margin-right: auto; font-family: "Source Sans Pro", helvetica, sans-serif; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
&lt;tr&gt;
&lt;th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1"&gt;&lt;div style="TRUE"&gt;y&lt;/div&gt;&lt;/th&gt;
&lt;th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="7"&gt;&lt;div style="TRUE"&gt;Sample Size (months)&lt;/div&gt;&lt;/th&gt;
&lt;th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1"&gt;&lt;div style="TRUE"&gt;Bench.&lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Horizon &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 6 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 12 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 24 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 36 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 60 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 84 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 120 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SP500 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 1M &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.086 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.088 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.072 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.078 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.078 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.081 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;"&gt; 0.098 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;"&gt; 0.054 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 3M &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.163 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.170 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.154 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.163 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.147 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.165 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;"&gt; 0.176 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;"&gt; 0.091 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6M &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.238 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.207 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.205 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.184 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.220 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;"&gt; 0.243 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;"&gt; 0.132 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 12M &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.203 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.177 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.206 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.220 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.222 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.317 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;"&gt; 0.286 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;"&gt; 0.189 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 24M &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.323 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.336 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.364 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.341 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.372 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.426 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;"&gt; 0.363 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;"&gt; 0.270 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



---
# Forecasting variance

This is easier ( `\(R^2&gt;0\)`). Low samples work better.

&lt;img src="variance.svg" width="800px" style="display: block; margin: auto;" /&gt;



---
# Conclusion

.font110[
1. The OOS **accuracy** of predictive regressions depends on some key parameters.    
2. The most important are sample size (T), forecasting horizon (k), and persistence of the dependent variable).   
3. The best results are obtained for small k and high persistence (e.g.: short-term volatility).   
4. If k is large, then the best hope is to reach the optimal loss, which is achieved with **very large samples**. 
]
The paper's **webpage**: http://www.gcoqueret.com/PRs.html
---

class: inverse, center, middle, animated, fadeInRight


&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 32px;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

# Thank you for your attention!


---

# Q&amp;A
  &lt;br&gt; &lt;br&gt; &lt;br&gt;
.font180[  What are your questions? ]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
