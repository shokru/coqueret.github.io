# Interpretability

In attempts to white-box complex machine learning models, one dichotomy stands out:

- **Global models** seek to determine the relative role of features in the construction of the prediction once the model has been trained.  
- **Local models** aim to characterise how the model behaves around one particular instance by considering small variations around this instance. The way these variations are handled by the original model allows to simplify it, i.e., approximate it (e.g., in a linear fashion) to determine the sign and magnitude of each relevant feature in the vicinity of the original instance.


## Global interpretations

Let us start with the simplest example of all. In a linear model, 
$$y_i=\alpha+\sum_{k=1}^K\beta_kx_i^k+\epsilon_i,$$
the following elements are scrutinized:

- the $R^2$, which appreciates the global fit of the model (possibly penalized to prevent overfitting with many regressors);   
- the sign of the estimates $\hat{\beta}_k$, which indicates the direction of the impact of each feature $x^k$ on $y$;
- the $t$-statistics $t_{\hat{\beta_k}}$, which evaluate the magnitude of this impact, regardless of its direction: large statistics in absolute value reveal prominent variables. 

The last two indicators are useful because they inform the user on which features matter the most and on the sign of the effect of each predictor. This gives a simplified view of how the model processes the features into the output. Most tools that aim to explain black boxes follow the same principles.


### Variable importance

One incredibly favorable feature of simple decision trees is their interpretability. Their visual representation is simple and straightforward. Just like regressions (which are another building block in ML), simple trees are simple to comprehend and do not suffer from the black-box rebuke that is often associated to more sophisticated tools.

Indeed, both random forests and boosted trees fail to provide perfectly accurate accounts of what is happening inside the engine. Nonetheless, it is possible to compute the aggregate share (or importance) of each feature in the determination of the structure of the tree once it has been trained.  

After training, it is possible to compute, at each node $n$ the gain $G(n)$ obtained by the subsequent split (if there are any, i.e., if the node is not a terminal leaf). It is also easy to determine which variable is chosen to perform the split to write $\mathcal{N}_f$ the set of nodes related to feature $f$. Then, the global importance of each feature is given by
$$I(f)=\sum_{n\in \mathcal{N}_f}G(n),$$
and it is often rescaled so that the sum of $I(f)$ across all $f$ is equal to one. In this case, $I(f)$ measures the relative contribution of feature $f$ in the reduction of loss during the training. A variable with high importance will have a greater impact on predictions. Generally, these variables are those that are located close to the root of the tree.



### Partial dependence plot



## Local interpretations

Whereas global interpretations seek to assess the impact of features on the output $overall$, local methods try to quantify the behavior of the model on particular instances or the neighborhood thereof.

### LIME

LIME (Local Interpretable Model-Agnostic Explanations) is a methodology originally proposed by @ribeiro2016should. Their aim is to provide
a faithfull account of the model under two constraints:

- \textbf{simple interpretability}, which implies a limited number of variables with visual or textual representation. This is to make sure any human can easily understand the outcome of the tool;
- \textbf{local faithfulness}: the explanation holds for the vicinity of the instance.

The original (black-box) model is $f$ and we assume we want to approximate its behavior around instance $x$ with the interpretable model $g$.\footnote{In the original paper, the authors dig deeper into the notion of interpretable representations. In complex machine learning settings (image recognition or natural language processing), the original features given to the model can be hard to interpret. Hence, this requires an additional translation layer because the outcome of LIME must be expressed in terms of easily understood quantities. In factor investing, the features are elementary, hence we do not need to deal with this issue).} The simple function $g$ belongs to a larger class $G$. The vicinity of $x$ is denoted $\pi_x$ and the complexity of $g$ is written $\Omega(g)$. LIME seeks an interpretation of the form
$$\xi(x)=\underset{g \in G}{\text{argmin}} \, \mathcal{L}(f,g,\pi_x)+\Omega(g),$$
where $\mathcal{L}(f,g,\pi_x)$ is the loss function (error/imprecision) induced by $g$ in the vicinity $\pi_x$ of $x$. The penalisation $\Omega(g)$ is for instance the number of leaves or depth of a tree, or the number of predictors in a linear regression.

It now remains to define some of the above terms. The vicinity of $x$ is defined by $\pi_x(z)=e^{-D(x,z)^2/\sigma^2},$ where $D$ is some distance measure. We underline that this function decreases when $z$ shifts away from $x$. 

The tricky part is the loss function. In order to minimise it, LIME generates artificial samples close to $x$ and averages/sums the error on the label that the simple representation makes. For simplicity, we assume a scalar output for $f$, hence the formulation is the following:
$$\mathcal{L}(f,g,\pi_x)=\sum_z \pi_x(z)(f(z)-g(z))^2$$
and the errors are weighted according to their distance from the initial instance $x$: the closest points get the largest weights. In its most basic implementation, the set of models $G$ consists of all linear models.

In Figure \@ref(fig:lime), we provide a simplified diagram of how LIME works. 


```{r lime, echo = FALSE, fig.cap = "Simplistic explanation of LIME.", out.width = "300px", fig.align="center"}
knitr::include_graphics("images/lime")
```


For expositional clarity, we work with only one dependent variable. The original training sample is shown with the black points. The fitted (trained) model is represented with the blue line and we want to approximate how the model works around one particular instance which is highlighted by the red square around it. In order to build the approximation, we sample 5 new points around the instance (the 5 red triangles). Each triangle lies on the blue line (they are model predictions) and has a weight proportional to its size: the triangle closest to the instance has a bigger weight. Using weighted least-squares, we build a linear model that fits to these 5 points. This is the outcome of the approximation. It gives the two parameters of the model: the intercept and the slope. Both can be evaluated with standard statistical tests. 

The sign of the slope is important. It is fairly clear that if the instance had been taken closer to $x=0$, the slope would have probably been almost flat and hence the predictor could be locally discarded. Another important detail is the number of sample points. In our explanation, we take only five, but in practice, a robust estimation usually requires around one thousand points or more. Indeed, when too few neighbors are sampled, the estimation risk is high and the approximation may be rough. 



### Shapley values
@shapley1953value
@lundberg2017unified
@chen2018shapley

### Breakdown